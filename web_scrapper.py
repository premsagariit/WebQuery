# -*- coding: utf-8 -*-
"""Web_Scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F5uGYGGp5ijDGzjXzzA19OewbJwJ1_r6
"""

# 1.1: Upgrade pip and install required libraries
!pip install --upgrade pip
!pip install transformers accelerate gradio

# 1.2: (Optional) If you need GPU support for faster inference
import torch
print("GPU available:", torch.cuda.is_available())

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

MODEL_NAME = "Qwen/Qwen3-4B"  # adjust if using a different repo

#2.1: Authenticate if private
from huggingface_hub import login
login(token="LOGIN_KEY") # <------------------------------------------------------- Create your own token at hugging face Token :)

# 2.2: Load tokenizer & model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype="auto",  # or float16 if supported
    trust_remote_code=True
)

# 2.3: Create a text-generation pipeline
llama_pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9
)

# 1. Install dependencies
!pip install --upgrade googlesearch-python newspaper3k lxml_html_clean

# 2. Imports
from googlesearch import search
from newspaper import Article
import logging # Import the logging module here


# 3. New web_search implementation
def web_search(query, max_results=5, snippet_chars=300):
    """
    1) Runs a Google search for `query`, returns up to `max_results` URLs.
    2) Uses newspaper3k to download & parse each article.
    3) Returns list of dicts: [{'title','href','body'}]
    """
    results = []
    for url in search(query, num_results=max_results, lang="en"):
        try:
            art = Article(url)
            art.download()
            art.parse()
            text = art.text.replace("\n", " ").strip()
            snippet = text[:snippet_chars] + ("â€¦" if len(text) > snippet_chars else "")
            results.append({
                "title": art.title or url,
                "href":  url,
                "body":  snippet
            })
        except Exception as e:
            logging.warning(f"Failed to parse {url}: {e}")
        if len(results) >= max_results:
            break
    return results

def answer_with_citations(user_query):
    # 4.1: Run our improved web search (news â†’ fallback text)
    hits = web_search(user_query, max_results=10)

    # 4.2: Build a prompt with numbered citations
    prompt_parts = [
        "You are an AI assistant. Use the following search results to answer the question below based on the best use of the information explain, list based on the question, and cite each source by the links\n"
    ]
    for idx, hit in enumerate(hits, start=1):
        title = hit.get("title", "").strip()
        body  = hit.get("body", "").strip()
        url   = hit.get("href", hit.get("url", "")).strip()
        prompt_parts.append(f"[{idx}] {title}\n{body}\nURL: {url}\n")
    prompt_parts.append(f"Question: {user_query}\nAnswer:")

    prompt = "\n".join(prompt_parts)

    # 4.3: Generate with LLaMA
    generated = llama_pipe(prompt, max_new_tokens=512, temperature=0.7)[0]["generated_text"]
    # Strip away the prompt to return just the answer portion:
    answer = generated[len(prompt):].strip()
    return answer

import gradio as gr

# 1) Update your chat function to work with history state
def chat_fn(user_message, history):
    # Append user message
    history = history + [(user_message, None)]
    # Generate bot response
    bot_reply = answer_with_citations(user_message)
    # Replace the None with the actual reply
    history[-1] = (user_message, bot_reply)
    return history, history

# 2) Some simple custom CSS for bubble styling
css = """
/* Center the chat on the page */
.gradio-container { display: flex; justify-content: center; padding: 2rem; }
/* Chat container */
.chat-box { width: 100%; max-width: 700px; }
/* User bubbles */
.chatbot .message.user {
  background-color: #007AFF;
  color: white;
  border-radius: 16px 16px 0 16px;
  padding: 8px 12px;
  margin: 4px 0;
  align-self: flex-end;
  max-width: 80%;
}
/* Bot bubbles */
.chatbot .message.bot {
  background-color: #FFA500;
  color: #111;
  border-radius: 16px 16px 16px 0;
  padding: 8px 12px;
  margin: 4px 0;
  align-self: flex-start;
  max-width: 80%;
}
/* Input box styling */
input[type="text"] {
  border-radius: 20px;
  padding: 10px 16px;
  border: 1px solid #CCC;
}
"""

with gr.Blocks(css=css, title="ðŸ“¡ LLaMAâ€¯3.2â€¯3B Webâ€‘Search Agent") as demo:
    gr.Markdown("## Ask anything and get cited answers")
    with gr.Group(elem_classes="chat-box"):
        chatbot = gr.Chatbot(elem_classes="chatbot", height=500)
        state   = gr.State([])  # holds the list of (user,bot) tuples
        user_input = gr.Textbox(
            placeholder="Type your question and hit enter",
            show_label=False
        )
    # wire up: on submit, call chat_fn(user_input, state) â†’ update chatbot & state
    user_input.submit(
        fn=chat_fn,
        inputs=[user_input, state],
        outputs=[chatbot, state]
    )

demo.launch(share=True)

